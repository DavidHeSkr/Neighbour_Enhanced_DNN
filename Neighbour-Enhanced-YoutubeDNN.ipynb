{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJSNi7ZQDh5x"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Ziyu He 184497359@qq.com\n",
    "Xuanji Xiao superwander@outlook.com\n",
    "\"\"\"\n",
    "\n",
    "%tensorflow_version 1.x\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "root_path = \"/Data/\"\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ORNa32T-xJd"
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(root_path + \"movies.dat\", sep=\"::\", header = None)\n",
    "rating_df = pd.read_csv(root_path + \"ratings.dat\", sep=\"::\", header = None)\n",
    "user_df = pd.read_csv(root_path + \"users.dat\", sep=\"::\", header = None)\n",
    "\n",
    "movie_df.columns = ['movie_id','title','genres']\n",
    "rating_df.columns = ['user_id','movie_id','rating','timestamp']\n",
    "user_df.columns = ['user_id','gender','age','occupation','zip']\n",
    "\n",
    "movie_df.genres = movie_df.genres.apply(lambda x: x.split(\"|\"))\n",
    "merged_df = pd.merge(pd.merge(rating_df,movie_df),user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-5CSdV7-xJf"
   },
   "outputs": [],
   "source": [
    "# Every user has watched at least 20 movies, can't simulate cold start, we\n",
    "# randomly select 1/10 history interaction data to similate cold start problem\n",
    "merged_df = merged_df.sample(frac = 0.1) \n",
    "hist_len = merged_df.user_id.value_counts()\n",
    "print(\"total user: {}\".format(len(hist_len)))\n",
    "# sparse user dara: every user has watched less than 30 movies\n",
    "sparse_hist_len = hist_len[hist_len < 30]\n",
    "print(\"sparse user: {}\".format(len(sparse_hist_len)))\n",
    "print(hist_len)\n",
    "print(\"total_interactions: {}\".format(sum(hist_len.values)))\n",
    "print(\"total_interactions_sparse: {}\".format(sum(sparse_hist_len.values)))\n",
    "print(\"=============\")\n",
    "print(sparse_hist_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU3Qfjxs-xJh"
   },
   "outputs": [],
   "source": [
    "#select sparse data\n",
    "merged_df = merged_df[merged_df.user_id.isin(sparse_hist_len.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCKKAKM8-xJj"
   },
   "outputs": [],
   "source": [
    "#make sure every user has watched at least 3 movies\n",
    "valid_user = merged_df.user_id.value_counts()[merged_df.user_id.value_counts() > 2].index\n",
    "merged_df['valid'] = merged_df.user_id.apply(lambda x: x in valid_user)\n",
    "merged_df = merged_df[merged_df.valid]\n",
    "merged_df.drop(['valid'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpqP_oOU-xJp"
   },
   "outputs": [],
   "source": [
    "merged_df.sort_values(\"timestamp\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMVOjjpR-xJr"
   },
   "outputs": [],
   "source": [
    "#search the static neighbours with the same attributes 【'gender', 'age', 'occupation'】\n",
    "def find_neighbour(uid, user_df):\n",
    "    select_ = user_df\n",
    "    user = select_[select_.user_id == uid]\n",
    "    for feature in ['gender', 'age', 'occupation']:\n",
    "        select_ = select_[select_[feature] == user[feature].values[0]]\n",
    "    return select_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmNRkZNR-xJt"
   },
   "outputs": [],
   "source": [
    "#the interest tag of the users \n",
    "from collections import Counter\n",
    "interest_dict = dict()\n",
    "for u, hist in merged_df.groupby('user_id'):\n",
    "    concat_ = np.concatenate(hist.genres.tolist())\n",
    "    interest_dict[u] = [x[0] for x in Counter(concat_).most_common(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j22XLH31-xJu"
   },
   "outputs": [],
   "source": [
    "#the users with the top 3 most common tags\n",
    "interest_dict\n",
    "def search_tag_neig(uid, id_tags, new):\n",
    "    ans = []\n",
    "    if new:\n",
    "        for i in id_tags:\n",
    "            if id_tags[i] == id_tags[uid]:\n",
    "                ans.append(i)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpPSu3W80P-3"
   },
   "outputs": [],
   "source": [
    "movie_df['genres_str'] = movie_df.genres.apply(lambda x:str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqEu3El0DlbH"
   },
   "outputs": [],
   "source": [
    "train_movie_df = movie_df[movie_df.movie_id.isin(train_merged_df.movie_id.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMDfD1npELOC"
   },
   "outputs": [],
   "source": [
    "movie_tag_dict = {}\n",
    "for i in range(train_movie_df.shape[0]):\n",
    "    if train_movie_df.iloc[i].genres_str in movie_tag_dict:\n",
    "        movie_tag_dict[train_movie_df.iloc[i].genres_str].append(train_movie_df.iloc[i].movie_id)\n",
    "    else:\n",
    "        movie_tag_dict[train_movie_df.iloc[i].genres_str] = [train_movie_df.iloc[i].movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jr_7D7l2FwKI"
   },
   "outputs": [],
   "source": [
    "movie_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGQ7Cj0TqKZ5"
   },
   "outputs": [],
   "source": [
    "movie_df.iloc[1].genres_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPstSGGZzjxx"
   },
   "outputs": [],
   "source": [
    "#item with the same tag\n",
    "def search_same_tag_movie(itemid, df):\n",
    "    target_item = df[df.movie_id == itemid]\n",
    "    target_genres = target_item.genres_str.unique()\n",
    "    return_list = []\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i]\n",
    "        movie_id = row.movie_id\n",
    "        genres = row.genres_str\n",
    "        #print(target_genres)\n",
    "        #print(genres)\n",
    "        if genres == target_genres:\n",
    "            return_list.append(movie_id)\n",
    "    return return_list\n",
    "\n",
    "same_tag_movie_dict = {}\n",
    "for i in tqdm(range(train_movie_df.shape[0])):\n",
    "    movie_id_ = train_movie_df.iloc[i]\n",
    "    same_tag_movie_dict[movie_id_.movie_id] = movie_tag_dict[movie_id_.genres_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5Ga6KNt-xJw"
   },
   "outputs": [],
   "source": [
    "# users with same attributes [gender、age、occpuation]\n",
    "tag_friends = []\n",
    "for i in merged_df.user_id.unique():\n",
    "    tag_friends.append(search_tag_neig(i, interest_dict, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tKGzhit-xJy"
   },
   "outputs": [],
   "source": [
    "tag_dict = {\n",
    "\"Drama\":\"0\",\n",
    "\"Comedy\":\"1\",\n",
    "\"Action\":\"2\",\n",
    "\"Thriller\":\"3\",\n",
    "\"Romance\":\"4\",\n",
    "\"Horror\":\"5\",\n",
    "\"Adventure\":\"6\",\n",
    "\"Sci-Fi\":\"7\",\n",
    "\"Children's\":\"8\",\n",
    "\"Crime\":\"9\",\n",
    "\"War\":\"10\",\n",
    "\"Documentary\":\"11\",\n",
    "\"Musical\":\"12\",\n",
    "\"Mystery\":\"13\",\n",
    "\"Animation\":\"14\",\n",
    "\"Fantasy\":\"15\",\n",
    "\"Western\":\"16\",\n",
    "\"Film-Noir\":\"17\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBKg0HIvABL_"
   },
   "outputs": [],
   "source": [
    "#delete the test dataset info to prevent data-leakage\n",
    "test_index = []\n",
    "for u_id, hist in merged_df.groupby('user_id'):\n",
    "    test_index.append(hist.iloc[-1].name)\n",
    "train_merged_df = merged_df[~merged_df.index.isin(test_index)]\n",
    "\n",
    "#create a itemcf user_item matrix\n",
    "max_item = max(train_merged_df.movie_id.unique() + 1)\n",
    "max_user = max(train_merged_df.user_id.unique() + 1)\n",
    "co_matrix = np.zeros((max_item,max_user))\n",
    "\n",
    "print(co_matrix.shape)\n",
    "for row_num in range(train_merged_df.shape[0]):\n",
    "    row = train_merged_df.iloc[row_num]\n",
    "    co_matrix[row.movie_id, row.user_id] = row.rating\n",
    "\n",
    "def cos_value(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "movie_id_list = train_merged_df.movie_id.value_counts().index\n",
    "simi_dict = {}\n",
    "for item in tqdm(movie_id_list):\n",
    "    temp_list = []\n",
    "    for other_item in movie_id_list:\n",
    "        temp_list.append((other_item, cos_value(co_matrix[item], co_matrix[other_item])))\n",
    "    temp_list = sorted(temp_list, key = lambda x:x[1], reverse = True)\n",
    "    temp_list = [x[0] for x in temp_list[:10]]\n",
    "    simi_dict[item] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GoFQzCpCHra"
   },
   "outputs": [],
   "source": [
    "len(same_tag_movie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97kXqpA3-xJz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess the dataset\n",
    "\"\"\"\n",
    "def gen_data_set(data, negsample):\n",
    "\n",
    "    data.sort_values(\"timestamp\", inplace=True)\n",
    "    item_ids = data['movie_id'].unique()\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['movie_id'].tolist()\n",
    "        rating_list = hist['rating'].tolist()\n",
    "        genres_list = hist['genres'].tolist()\n",
    "\n",
    "        #负采样negsample个\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
    "        \n",
    "        #the neighbours with the same characteristics(gender\\age\\occupation)\n",
    "        neighbours = find_neighbour(reviewerID, user_df).user_id.values\n",
    "        \n",
    "        #the neighbours with the same tags(Comedy\\Children\\Drama\\...)\n",
    "        tag_neighbours = search_tag_neig(reviewerID, interest_dict, True)\n",
    "        \n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            \n",
    "            #the similar movie of the predicted movie (except: the movie only exists in test data)\n",
    "            try:\n",
    "                total_similar = simi_dict[pos_list[i]]\n",
    "            except:\n",
    "                total_similar = []\n",
    "\n",
    "            # movies with the same tag of the predicted movie\n",
    "            #tag_similar_movie = search_same_tag_movie(pos_list[i], movie_df)\n",
    "            try:\n",
    "                tag_similar_movie = same_tag_movie_dict[pos_list[i]]\n",
    "            except:\n",
    "                print(pos_list[i])\n",
    "                tag_similar_movie = []\n",
    "            \n",
    "            #the similar movies of all history movies (except: the movie only exists in test data)\n",
    "            hist_similar = []\n",
    "            for hist_movie in hist:\n",
    "                try:\n",
    "                    hist_similar = hist_similar + simi_dict[hist_movie]\n",
    "                except:\n",
    "                    hist_similar = hist_similar + []\n",
    "            \n",
    "            #the tag_info of the predicted vedio\n",
    "            vedio_tag = [tag_dict[x] for x in genres_list[i]]\n",
    "            \n",
    "            #the tag_info of all history movies\n",
    "            tags = np.concatenate(genres_list[:i])\n",
    "            tags_ = [tag_dict[x] for x in tags]\n",
    "            \n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((reviewerID, hist, pos_list[i], 1, len(hist[::-1]), neighbours, tag_neighbours, tags_, vedio_tag, total_similar, hist_similar, tag_similar_movie))\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, hist, neg_list[i*negsample+negi], 0,len(hist[::-1]),neighbours, tag_neighbours, tags_, vedio_tag, total_similar, hist_similar, tag_similar_movie))\n",
    "            else:\n",
    "                test_set.append((reviewerID, hist, pos_list[i], 1,len(hist[::-1]),neighbours, tag_neighbours, tags_, vedio_tag, total_similar, hist_similar, tag_similar_movie))\n",
    "                for negi in range(negsample):\n",
    "                    test_set.append((reviewerID, hist, neg_list[i*negsample+negi], 0,len(hist[::-1]),neighbours, tag_neighbours, tags_, vedio_tag, total_similar, hist_similar, tag_similar_movie))\n",
    "\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print(len(train_set[0]),len(test_set[0]))\n",
    "    return train_set,test_set\n",
    "\n",
    "def gen_model_input(train_set,user_profile):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "    train_tag_neighbours = np.array([line[5] for line in train_set])\n",
    "    train_neighbours = np.array([line[6] for line in train_set])\n",
    "    train_tags = np.array([line[7] for line in train_set])\n",
    "    movie_tags = np.array([line[8] for line in train_set])\n",
    "    similari_movies = np.array([line[9] for line in train_set])\n",
    "    hist_similar = np.array([line[10] for line in train_set])\n",
    "    tag_similar_movie = np.array([line[11] for line in train_set])\n",
    "    \n",
    "    train_model_input = {\"label\": train_label, \"user_id\": train_uid, \"movie_id\": train_iid,\n",
    "                         \"hist_movie_id\": train_seq, \"hist_len\": train_hist_len, \"neighbours\": train_neighbours, \n",
    "                         \"tags\":train_tags, \"movie_tag\": movie_tags, \"similar_movies\":similari_movies, \n",
    "                         \"history_similar_movies\":hist_similar, \"tag_neighbours\": train_tag_neighbours, \"tag_similar_movie\":tag_similar_movie}\n",
    "\n",
    "    for key in [\"gender\", \"age\", \"occupation\"]:\n",
    "        #transfer the uid to the index in user_df\n",
    "        train_model_input[key] = user_profile.iloc[train_model_input['user_id'] - 1][key].values\n",
    "\n",
    "    return train_model_input\n",
    "\n",
    "                                             #3 negative samples \n",
    "train_set, test_set = gen_data_set(merged_df, 3)\n",
    "train_dataset = gen_model_input(train_set, user_df)\n",
    "test_dataset = gen_model_input(test_set, user_df)\n",
    "\n",
    "print(train_dataset.keys())\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "with open(root_path + 'train_data.json', 'w') as outfile:\n",
    "    json.dump(train_dataset, outfile, cls=NumpyEncoder)\n",
    "    \n",
    "with open(root_path + 'test_data.json', 'w') as outfile:\n",
    "    json.dump(test_dataset, outfile, cls=NumpyEncoder)\n",
    "\n",
    "data_fea = list(train_dataset.keys())\n",
    "print(train_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w4aGfU5-xJ5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transfer to tfrecord\n",
    "\"\"\"\n",
    "\n",
    "class handle_data():\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.data_fea = list(dataset.keys())\n",
    "        self._get_next_data = self._get_single_data()\n",
    "    \n",
    "    def _get_single_data(self):\n",
    "        data_len = len(self.dataset['user_id'])\n",
    "        data_dict = dict()\n",
    "        i = 0\n",
    "        while i < data_len:\n",
    "            for fea in self.data_fea:\n",
    "                data_dict[fea] = self.dataset[fea][i]\n",
    "            yield data_dict\n",
    "            i += 1\n",
    "          \n",
    "    @property\n",
    "    def get_next_data(self):\n",
    "        return self._get_next_data\n",
    "    \n",
    "    @property\n",
    "    def create_next_example(self):\n",
    "        \"\"\"\n",
    "        'label', 'user_id', 'movie_id', 'hist_movie_id', 'hist_len', \n",
    "        'neighbours', 'tags', 'movie_tag', 'similar_movies', 'history_similar_movies',\n",
    "        'tag_neighbours', 'gender', 'age', 'occupation'\n",
    "        \"\"\"\n",
    "        \n",
    "        next_ = next(self.get_next_data)\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            \n",
    "            'user_id': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['user_id']).encode('utf-8')])),        \n",
    "            \n",
    "            'user_id_offline': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['user_id']).encode('utf-8')])),\n",
    "            \n",
    "            'user_id_offline_fixed': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['user_id']).encode('utf-8')])),\n",
    "            \n",
    "            'movie_id': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['movie_id']).encode('utf-8')])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'hist_movie_id': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['hist_movie_id']])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'neighbours': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['neighbours']])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'tag_neighbours': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['tag_neighbours']])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'user_tags': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['tags']])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'movie_tags': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['movie_tag']])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'similar_movies': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['similar_movies']])),\n",
    "            \n",
    "            # multi-value feature\n",
    "            'history_similar_movies': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['history_similar_movies']])),\n",
    "\n",
    "            # multi-value feature\n",
    "            'tag_similar_movie': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(m).encode('utf-8') for m in next_['tag_similar_movie']])),\n",
    "            \n",
    "            'hist_len': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['hist_len']).encode('utf-8')])),\n",
    "            \n",
    "            'gender': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['gender']).encode('utf-8')])),\n",
    "            \n",
    "            'age' : tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['age']).encode('utf-8')])),\n",
    "            \n",
    "            'occupation': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['occupation']).encode('utf-8')])),\n",
    "            \n",
    "            'label': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(\n",
    "                    value=[str(next_['label']).encode('utf-8')])),\n",
    "            \n",
    "        }))\n",
    "        return example\n",
    "\n",
    "train_data_handler = handle_data(train_dataset)\n",
    "test_data_handler = handle_data(test_dataset)\n",
    "\n",
    "with tf.io.TFRecordWriter(root_path + 'train.tfrecord') as writer:\n",
    "    while True:\n",
    "        try:\n",
    "            writer.write(train_data_handler.create_next_example.SerializeToString())\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "with tf.io.TFRecordWriter(root_path + 'test.tfrecord') as writer:\n",
    "    while True:\n",
    "        try:\n",
    "            writer.write(test_data_handler.create_next_example.SerializeToString())\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eVJyOzE-xJ8"
   },
   "outputs": [],
   "source": [
    "def _join_pipeline(map_functions):\n",
    "\n",
    "    def joined_map_fn(example):\n",
    "        for map_fn in map_functions:\n",
    "            example = map_fn(example)\n",
    "        return example\n",
    "\n",
    "    return joined_map_fn\n",
    "\n",
    "def _dataset_map(dataset, map_functions):\n",
    "    if len(map_functions) > 0:\n",
    "        map_fn = _join_pipeline(map_functions)\n",
    "        dataset = dataset.map(map_fn)\n",
    "    return dataset\n",
    "\n",
    "def _transform(example):\n",
    "    read_features = dict()\n",
    "    for key in ['user_tags', 'movie_tags', 'user_id_offline', 'user_id_offline_fixed', 'age', 'gender', 'hist_movie_id', 'label', 'movie_id', 'neighbours', 'occupation', 'user_id', 'similar_movies', 'history_similar_movies', 'tag_neighbours','tag_similar_movie']:\n",
    "        if key == 'hist_movie_id' or key == 'neighbours' or key == 'user_tags' or key == 'movie_tags' or key == 'similar_movies' or key == 'history_similar_movies' or key == 'tag_neighbours' or key == 'tag_similar_movie':\n",
    "            read_features[key] = tf.FixedLenSequenceFeature([],dtype=tf.string,allow_missing=True)\n",
    "        else:\n",
    "            read_features[key] = tf.FixedLenFeature([], dtype=tf.string)\n",
    "\n",
    "    # Extract features from serialized data\n",
    "    read_data = tf.parse_example(serialized=example,\n",
    "                                        features=read_features)\n",
    "\n",
    "    for k, v in read_data.items():\n",
    "        read_data[k] = tf.identity(v, name=k)\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLq8APni-xJ-"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-2kSDUU-xJ_"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import os\n",
    "import time\n",
    "\n",
    "# create the dataset\n",
    "dataset = tf.data.TFRecordDataset('train.tfrecord')\n",
    "dataset = dataset.batch(1000, True)\n",
    "dataset = _dataset_map(dataset, [_transform])\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset('test.tfrecord')\n",
    "test_dataset = test_dataset.batch(1000, True)\n",
    "test_dataset = _dataset_map(test_dataset, [_transform])\n",
    "test_iterator = test_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUpbTevB-xKB"
   },
   "outputs": [],
   "source": [
    "class NumericalStat(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._min = float(\"inf\")\n",
    "        self._max = float(\"-inf\")\n",
    "        self._sum = 0.0\n",
    "        self._square_sum = 0.0\n",
    "        self._count = 0\n",
    "\n",
    "    def update(self, values):\n",
    "        assert np.issubdtype(values.dtype, np.number)\n",
    "        self._min = min(np.min(values), self._min)\n",
    "        self._max = max(np.max(values), self._max)\n",
    "        self._sum += np.sum(values)\n",
    "        self._square_sum += np.sum(values * values)\n",
    "        self._count += values.size\n",
    "\n",
    "    @property\n",
    "    def min(self):\n",
    "        return self._min\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return self._max\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._sum / float(self._count)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return (self._square_sum / float(self._count) - self.mean * self.mean) ** 0.5\n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        return self._count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"stat(type=numerical, min=%f, max=%f, mean=%f, std=%f, n=%d)\" % (\n",
    "            self.min,\n",
    "            self.max,\n",
    "            self.mean,\n",
    "            self.std,\n",
    "            self.n_samples,\n",
    "        )\n",
    "\n",
    "\n",
    "class CategoricalStat(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._values = defaultdict(int)\n",
    "        self._count = 0\n",
    "\n",
    "    def update(self, values):\n",
    "        for value in values.flatten():\n",
    "            self._values[value] += 1\n",
    "        self._count += values.size\n",
    "\n",
    "    def values_top_k(self, top_k=None):\n",
    "        sorted_values = [\n",
    "            k for k, v in sorted(self._values.items(), key=lambda item: item[1])\n",
    "        ]\n",
    "        if top_k is None:\n",
    "            return sorted_values\n",
    "        else:\n",
    "            return sorted_values[:top_k]\n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        return self._count\n",
    "\n",
    "    @property\n",
    "    def total_values(self):\n",
    "        return len(self._values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"stat(type=categorical, top-5-values=%s, #values=%d, n=%d)\" % (\n",
    "            self.values_top_k(5),\n",
    "            self.total_values,\n",
    "            self.n_samples,\n",
    "        )\n",
    "\n",
    "\n",
    "class Statistics(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._stats = dict()\n",
    "\n",
    "    def update(self, name, values):\n",
    "        if np.issubdtype(values.dtype, np.number):\n",
    "            if name not in self._stats:\n",
    "                self._stats[name] = NumericalStat()\n",
    "            self._stats[name].update(values)\n",
    "        else:\n",
    "            if name not in self._stats:\n",
    "                self._stats[name] = CategoricalStat()\n",
    "            self._stats[name].update(values)\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        return self._stats\n",
    "\n",
    "    def save_to_file(self, filepath):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(self._stats, f)\n",
    "\n",
    "    def load_from_file(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            self._stats = pickle.load(f)\n",
    "\n",
    "    def load_from_textfile(self, filepath, threshold = 0):\n",
    "        vocab = defaultdict(list)\n",
    "        with open(filepath, \"rt\", encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                fea_cat, fea_value, cnt = line.strip().split(\"\\t\")\n",
    "                if int(cnt) > threshold:\n",
    "                    vocab[fea_cat].append(fea_value)\n",
    "        for name in vocab:\n",
    "            self.update(name, np.array(vocab[name]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\"%s: %s\" % (name, stat) for name, stat in self.stats.items()])\n",
    "\n",
    "\n",
    "import abc\n",
    "\n",
    "class BaseStatisticsGen(metaclass=abc.ABCMeta):\n",
    "    \"\"\"Base class for a statistics generator component.\n",
    "\n",
    "    All subclasses of BaseStatisticsGen must override `run` method to generate\n",
    "    feauture statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def run(self) -> Statistics:\n",
    "        \"\"\"Generate feature statistics.\n",
    "\n",
    "        :return: Feature statistics results.\"\"\"\n",
    "        pass\n",
    "\n",
    "from collections import defaultdict\n",
    "class DatasetStatisticsGen(BaseStatisticsGen):\n",
    "\n",
    "    def __init__(self, iterator, num_batches=None):\n",
    "        self.iterator = iterator\n",
    "        self._num_batches = num_batches\n",
    "\n",
    "    def run(self) -> Statistics:\n",
    "        sess = tf.Session()\n",
    "        self.iterator.initializer.run(session = sess)\n",
    "        #self._dataset.init(sess)\n",
    "        statistics = Statistics()\n",
    "        n = 0\n",
    "        while True:\n",
    "            try:\n",
    "                batch = sess.run(self.iterator.get_next())\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            #print(batch)\n",
    "            for name, values in batch.items():\n",
    "                statistics.update(name, values)\n",
    "            n += 1\n",
    "            if n % 1000 == 0:\n",
    "                print(\"Statistics collected for %d batches ...\" % n)\n",
    "            if self._num_batches is not None and n >= self._num_batches:\n",
    "                break\n",
    "        sess.close()\n",
    "        print(\"Statistics collected for %d batches in total.\" % n)\n",
    "        return statistics\n",
    "\n",
    "data_statistic = DatasetStatisticsGen(iterator)\n",
    "statistic = data_statistic.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37tv66ei-xKC"
   },
   "outputs": [],
   "source": [
    "len(statistic.stats[\"movie_id\"].values_top_k(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wip5n1e1-xKE"
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "from typing import Callable\n",
    "\n",
    "class BaseTransform(metaclass=abc.ABCMeta):\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def transform_fn(self) -> Callable[[tf.train.Example], tf.train.Example]:\n",
    "        \"\"\"Returns the transform function.\n",
    "\n",
    "        :return: A function to transform `tf.Example`.\"\"\"\n",
    "        return self._transform_fn\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _transform_fn(self, example: tf.train.Example) -> tf.train.Example:\n",
    "        \"\"\"Transforms `tf.Example`.\n",
    "\n",
    "        :param example: The input `tf.Example` to be transformed.\n",
    "        :return: The transformed `tf.Example` as output.\"\"\"\n",
    "        return example\n",
    "\n",
    "map_num_oov_buckets = {}\n",
    "map_top_k_to_select = {}\n",
    "map_shared_embedding = {\"similar_movies\" : \"movie_id\", \"hist_movie_id\" : \"movie_id\", \"neighbours\":\"user_id\", \"user_id_offline_fixed\":\"user_id_offline\", \"user_tags\":\"movie_tags\", \"history_similar_movies\":\"movie_id\", \"tag_neighbours\":\"user_id\", \"tag_similar_movie\":\"movie_id\"}\n",
    "\n",
    "class CategoricalTransform(BaseTransform):\n",
    "\n",
    "    def __init__(self,\n",
    "                 statistics,\n",
    "                 feature_names,\n",
    "                 embed_size=32,\n",
    "                 default_num_oov_buckets=1,\n",
    "                 map_num_oov_buckets={},\n",
    "                 map_top_k_to_select={},\n",
    "                 map_shared_embedding={},\n",
    "                 scope_name=\"categorical_transform\"):\n",
    "        self._statistics = statistics\n",
    "        self._feature_names = feature_names\n",
    "        self._default_num_oov_buckets = default_num_oov_buckets\n",
    "        self._map_num_oov_buckets = map_num_oov_buckets\n",
    "        self._map_top_k_to_select = map_top_k_to_select\n",
    "        self._map_shared_embedding = map_shared_embedding\n",
    "        self._embed_size = embed_size\n",
    "        self._scope_name = scope_name\n",
    "        self._hash_tables, self.hash_sizes = self._create_hash_tables()\n",
    "        self._embedding_tables = self._create_embedding_tables(self.hash_sizes)\n",
    "\n",
    "    def _transform_fn(self, example):\n",
    "        example = self._hash_lookup(example)\n",
    "        example = self._embedding_lookup(example)\n",
    "        return example\n",
    "\n",
    "    def _hash_lookup(self, example):\n",
    "        for fea_name in self._feature_names:\n",
    "            # keep the \"user_id_offline_fixed\" unchanged\n",
    "            if fea_name == \"user_id_offline_fixed\":\n",
    "                continue\n",
    "            if fea_name not in self._map_shared_embedding: \n",
    "                example[fea_name] = self._hash_tables[fea_name].lookup(\n",
    "                    example[fea_name]\n",
    "                )\n",
    "            else:\n",
    "                shared_fea_name = self._map_shared_embedding[fea_name]\n",
    "                example[fea_name] = self._hash_tables[shared_fea_name].lookup(\n",
    "                    example[fea_name]\n",
    "                )\n",
    "        return example\n",
    "\n",
    "    def _embedding_lookup(self, example):\n",
    "        for fea_name in self._feature_names:\n",
    "            # keep the \"user_id_offline_fixed\" unchanged\n",
    "            if fea_name == \"user_id_offline_fixed\":\n",
    "                continue\n",
    "            if fea_name not in self._map_shared_embedding:\n",
    "                example[fea_name] = tf.nn.embedding_lookup(\n",
    "                    self._embedding_tables[fea_name], example[fea_name]\n",
    "                )\n",
    "            else:\n",
    "                shared_fea_name = self._map_shared_embedding[fea_name]\n",
    "                example[fea_name] = tf.nn.embedding_lookup(\n",
    "                    self._embedding_tables[shared_fea_name], example[fea_name]\n",
    "                )\n",
    "        return example\n",
    "    \n",
    "    def _embedding_modify_fea(self, example, fea_name, value):\n",
    "\n",
    "        if fea_name not in self._map_shared_embedding:\n",
    "        #print('self.uid_offlines', self.uid_offlines)\n",
    "        #print('example[fea_name]', example[fea_name])\n",
    "            #index_ = example[fea_name]\n",
    "            print(\"changed fea_name:\", fea_name)\n",
    "            index_ = tf.transpose([example[fea_name]], perm=[1, 0])\n",
    "            print(\"changed_index_\", index_)\n",
    "            print(\"original self._embedding_tables[fea_name]\", self._embedding_tables[fea_name])\n",
    "            self._embedding_tables[fea_name] = tf.scatter_nd_update(self._embedding_tables[fea_name], index_, value)\n",
    "            print(\"value\", value)\n",
    "            print(\"modified self._embedding_tables[fea_name]\", self._embedding_tables[fea_name])\n",
    "            #select = self._embedding_tables[fea_name][example[fea_name], :]\n",
    "            #select.assign(value)\n",
    "            \n",
    "        else:\n",
    "            shared_fea_name = self._map_shared_embedding[fea_name]\n",
    "            tf.assign(self._embedding_tables[shared_fea_name][example[shared_fea_name]], value)\n",
    "    \n",
    "    def _hash_lookup_fea(self, example, fea_name):\n",
    "        if fea_name not in self._map_shared_embedding: \n",
    "                example[fea_name] = self._hash_tables[fea_name].lookup(\n",
    "                    example[fea_name]\n",
    "                )\n",
    "        else:\n",
    "            # find the hash_id of the user_id_offline by looking up the hash value of user_id_offline_fixed  \n",
    "            shared_fea_name = self._map_shared_embedding[fea_name]            \n",
    "            print(\"fea_name\", example[fea_name])\n",
    "            print(\"shared_fea_name\", example[shared_fea_name])\n",
    "            \n",
    "            example[shared_fea_name] = self._hash_tables[shared_fea_name].lookup(\n",
    "                example[fea_name]\n",
    "            )\n",
    "        return example\n",
    "        \n",
    "    # modi_fea = 'user_id_offline'\n",
    "    def modify_embedding(self, example, value, modi_fea = 'user_id_offline_fixed'):\n",
    "        example = self._hash_lookup_fea(example, modi_fea)\n",
    "        modi_fea = 'user_id_offline'\n",
    "        self._embedding_modify_fea(example, modi_fea, value)\n",
    "        \n",
    "\n",
    "    def _create_hash_tables(self):\n",
    "        hash_tables = {}\n",
    "        hash_sizes = {}\n",
    "        for fea_name in self._feature_names:\n",
    "            if fea_name in self._map_shared_embedding:\n",
    "                assert self._map_shared_embedding[fea_name] in self._feature_names\n",
    "            else:\n",
    "                num_oov_buckets = (\n",
    "                    self._map_num_oov_buckets[fea_name]\n",
    "                    if fea_name in self._map_num_oov_buckets\n",
    "                    else self._default_num_oov_buckets\n",
    "                )\n",
    "                top_k = (\n",
    "                    self._map_top_k_to_select[fea_name]\n",
    "                    if fea_name in self._map_top_k_to_select\n",
    "                    else None\n",
    "                )\n",
    "                vocab = []\n",
    "                print(\"fea name:\",fea_name)\n",
    "                \n",
    "                if fea_name in self._statistics.stats:\n",
    "                    vocab = self._statistics.stats[fea_name].values_top_k(top_k)\n",
    "                \n",
    "                else:\n",
    "                    print(\n",
    "                        \"WARNING: feature [%s] not found in statistics, use empty.\"\n",
    "                        % fea_name\n",
    "                    )\n",
    "                #print(\"vocab:\",vocab)\n",
    "                hash_tables[fea_name] = tf.contrib.lookup.index_table_from_tensor(\n",
    "                    mapping=tf.constant(vocab), num_oov_buckets=num_oov_buckets\n",
    "                )\n",
    "                hash_sizes[fea_name] = len(vocab) + num_oov_buckets\n",
    "        return hash_tables, hash_sizes\n",
    "\n",
    "    def _create_embedding_tables(self, hash_sizes):\n",
    "        embedding_tables = {}\n",
    "        with tf.variable_scope(self._scope_name, reuse=tf.AUTO_REUSE):\n",
    "            for fea_name in self._feature_names:\n",
    "                if fea_name in self._map_shared_embedding:\n",
    "                    assert self._map_shared_embedding[fea_name] in self._feature_names\n",
    "                else:\n",
    "                    embedding_tables[fea_name] = tf.get_variable(\n",
    "                        fea_name + \"_embed\", [hash_sizes[fea_name], self._embed_size]\n",
    "                    )\n",
    "        return embedding_tables\n",
    "\n",
    "#list(train_dataset.keys())\n",
    "cate_trans = CategoricalTransform(statistics = statistic, feature_names = ['user_tags', 'movie_tags','user_id','user_id_offline','user_id_offline_fixed','movie_id','hist_movie_id','gender','age','occupation', 'neighbours', \"similar_movies\",'history_similar_movies','tag_neighbours','tag_similar_movie'], map_num_oov_buckets=map_num_oov_buckets, map_top_k_to_select=map_num_oov_buckets, map_shared_embedding = map_shared_embedding)\n",
    "#cate_trans._transform_fn(iterator.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI4HaFq8-xKG"
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "class BaseNetwork(metaclass=abc.ABCMeta):\n",
    "    \"\"\"Base class for a neural network component.\n",
    "\n",
    "    All subclasses of BaseNetwork must override _train_fn, _eval_fn and _serve_fn\n",
    "    methods. _train_fn builds the training graph and returns a loss `tf.Tensor`.\n",
    "    _eval_fn builds the evaluation graph and outputs a inference `tf.Tensor`.\n",
    "    _serve_fn might be the same to _eval_fn or do additional graph surgery for\n",
    "    efficient online serving.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def train_fn(self):\n",
    "        \"\"\"Returns a function to build training graph.\n",
    "\n",
    "        :return: A function to build training graph (loss as output).\"\"\"\n",
    "        return self._train_fn\n",
    "\n",
    "    @property\n",
    "    def eval_fn(self):\n",
    "        \"\"\"Returns a function to build inference graph.\n",
    "\n",
    "        :return: A function to build inference graph (inference result as output).\"\"\"\n",
    "        return self._eval_fn\n",
    "\n",
    "    @property\n",
    "    def serve_fn(self):\n",
    "        \"\"\"Returns a function to build serving graph.\n",
    "\n",
    "        :return: A function to build serving graph (inference result as output).\"\"\"\n",
    "        return self._serve_fn\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _train_fn(self, example: tf.train.Example) -> tf.Tensor:\n",
    "        \"\"\"Build training graph.\n",
    "\n",
    "        :param example: The `tf.Example` used as graph input.\n",
    "        :return: A loss `tf.Tensor`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def serve_inputs(self):\n",
    "        \"\"\"Returns a function to offer inputs for serving.\n",
    "\n",
    "        :return: A function for offering inputs.\n",
    "        \"\"\"\n",
    "        return self._get_serve_inputs()\n",
    "\n",
    "    def _tile_tensor_with_batch_size(self, tensor, batch_size):\n",
    "        \"\"\"Tile tensor for optimized model serving.\n",
    "\n",
    "        Tile the tensor to [batch_size, 1] if its first dimension is 1.\n",
    "        This is used for optimized model serving,\n",
    "        which has squeezed user/context feature inputs (batch_size = 1)\n",
    "        and unsqueezed item feature inputs (batch_size > 1).\n",
    "\n",
    "        :param tensor: Input tensor.\n",
    "        :param batch_size: Target batch size.\n",
    "        :return: Tiled tensor with first dimension equal to `batch_size`.\n",
    "        \"\"\"\n",
    "        dims = len(tensor.get_shape().as_list())\n",
    "        shape = [1] * dims\n",
    "        shape[0] = batch_size\n",
    "        output = tf.cond(tf.equal(tf.shape(tensor)[0], 1),\n",
    "                         lambda: tf.tile(tensor, shape),\n",
    "                         lambda: tensor)\n",
    "        return output\n",
    "\n",
    "    def _tile_tensors_with_batch_size(self, tensors, batch_size):\n",
    "        \"\"\"Tile tensors for optimized model serving.\n",
    "\n",
    "        For every tensor of tensors,\n",
    "        do _tile_tensor_with_batch_size(tensor, batch_size).\n",
    "\n",
    "        :param tensors: Input tensors list.\n",
    "        :param batch_size: Target batch size.\n",
    "        :return: Tiled tensors with first dimension of\n",
    "        every tensor equal to `batch_size`.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self._tile_tensor_with_batch_size(tensor, batch_size)\n",
    "            for tensor in tensors\n",
    "        ]\n",
    "\n",
    "    def _get_batch_size(self, example):\n",
    "        \"\"\"Get the batch size.\n",
    "\n",
    "        Get the batch size by traversing all features' shape in example.\n",
    "\n",
    "        :param example: Dict of string->tensor.\n",
    "        :return: A tensor of batch_size.\n",
    "        \"\"\"\n",
    "        batch_size_tensors = [tf.shape(tensor)[0] for key, tensor in example.items()\n",
    "                              if key in self.serve_inputs]\n",
    "        batch_size = tf.reduce_max(batch_size_tensors)\n",
    "        return batch_size\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "class MlpNetwork(BaseNetwork):\n",
    "\n",
    "    def __init__(self,\n",
    "                 train,\n",
    "                 cate_trans,\n",
    "                 item_features,\n",
    "                 categorical_features,\n",
    "                 numerical_features,\n",
    "                 multivalue_features,\n",
    "                 loss,\n",
    "                 hidden_sizes=[1024, 512, 128],\n",
    "                 scope_name=\"mlp_network\", ptype = \"mean\"):\n",
    "        self._train = train\n",
    "        self.item_features = item_features\n",
    "        self.cate_trans = cate_trans\n",
    "        self._categorical_features = categorical_features\n",
    "        self._numerical_features = numerical_features\n",
    "        self._multivalue_features = multivalue_features\n",
    "        self._loss = loss\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        self._scope_name = scope_name\n",
    "        self._ptype = ptype\n",
    "\n",
    "    def _train_fn(self, example):\n",
    "        with tf.variable_scope(self._scope_name, reuse=tf.AUTO_REUSE):\n",
    "            logits = self._build_graph(example)\n",
    "            loss = self._loss.loss_fn(logits, example)\n",
    "            labels = tf.expand_dims(tf.to_int32(tf.strings.to_number(example['label'])), 1, name=None) \n",
    "            return loss, logits, labels\n",
    "\n",
    "    def _eval_fn(self, example):\n",
    "        with tf.variable_scope(self._scope_name, reuse=tf.AUTO_REUSE):\n",
    "            logits = self._build_graph(example)\n",
    "            outputs = tf.sigmoid(logits)\n",
    "            return outputs\n",
    "\n",
    "    def _serve_fn(self, example):\n",
    "        return self._eval_fn(example)\n",
    "    \n",
    "    \n",
    "    def _build_multivalue_part(self, inputs):\n",
    "        def pooling(vals, ptype, fea_name):\n",
    "            if ptype == \"mean\":\n",
    "                return tf.reduce_mean(vals, axis=1)\n",
    "            elif ptype == \"sum\":\n",
    "                return tf.reduce_sum(vals, axis=1)\n",
    "            elif ptype == \"fc\":\n",
    "                return tf.squeeze(slim.fully_connected(\n",
    "                    tf.transpose(vals, [0, 2, 1]),\n",
    "                    1,\n",
    "                    scope=\"multivalue_fc_%s\" % fea_name))\n",
    "\n",
    "        outputs = [pooling(inputs[name], self._ptype, name) \n",
    "            for name in self._multivalue_features]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def _build_item_part(self, inputs):\n",
    "        def pooling(vals, ptype, fea_name):\n",
    "            if ptype == \"mean\":\n",
    "                return tf.reduce_mean(vals, axis=1)\n",
    "            elif ptype == \"sum\":\n",
    "                return tf.reduce_sum(vals, axis=1)\n",
    "            elif ptype == \"fc\":\n",
    "                return tf.squeeze(slim.fully_connected(\n",
    "                    tf.transpose(vals, [0, 2, 1]),\n",
    "                    1,\n",
    "                    scope=\"multivalue_fc_%s\" % fea_name))\n",
    "\n",
    "        outputs = [pooling(inputs[name], self._ptype, name) \n",
    "            for name in self.item_features]\n",
    "        return outputs\n",
    "    \n",
    "    def build_HAN(self, inputs, feature, target_feature, attention_enhance, trans_embed = 16):\n",
    "        #user HAN enhance\n",
    "        if attention_enhance:\n",
    "            neighbours = inputs[feature]\n",
    "            \n",
    "            trans_W = tf.get_variable(\"trans_W\", [inputs[target_feature].shape[-1], trans_embed], trainable = True)\n",
    "            \n",
    "            q_1 = tf.get_variable(\"q_1\", [trans_embed, 1], trainable = True)\n",
    "            q_2 = tf.get_variable(\"q_2\", [trans_embed, 1], trainable = True)\n",
    "            expand_self = tf.expand_dims(inputs[target_feature], 1)\n",
    "            \n",
    "            neighbours_ = tf.matmul(neighbours, trans_W)\n",
    "            expand_self_ = tf.matmul(expand_self, trans_W)\n",
    "            \n",
    "            f_1 = tf.matmul(neighbours_, q_1)\n",
    "            self_node = tf.matmul(expand_self_, q_2)\n",
    "\n",
    "            logits = self_node + f_1\n",
    "            logits = tf.squeeze(logits, [-1])\n",
    "            coefs = tf.nn.softmax(tf.nn.leaky_relu(logits))\n",
    "            coefs_ = tf.expand_dims(coefs, axis = 1)\n",
    "            vals = tf.matmul(coefs_, neighbours_)\n",
    "            vals = tf.squeeze(vals, axis = 1)\n",
    "\n",
    "        else:\n",
    "            vals = tf.reduce_mean(inputs[feature], axis = 1)\n",
    "        #ret = tf.contrib.layers.bias_add(vals)\n",
    "        return vals\n",
    "        \n",
    "\n",
    "    def _build_graph(self, inputs):\n",
    "        \n",
    "        user_enhance = self.build_HAN(inputs, 'neighbours','user_id', False)\n",
    "        item_enhance = self.build_HAN(inputs, 'similar_movies', 'movie_id', False)\n",
    "        \n",
    "        item_tags = tf.concat(self._build_item_part(inputs), axis = 1, name = \"concat_0\")\n",
    "        multivalue_features_ = tf.concat(self._build_multivalue_part(inputs), axis = 1, name = \"concat_1\")\n",
    "        \n",
    "        categorical_part = tf.concat(\n",
    "            [inputs[name] for name in self._categorical_features],\n",
    "            axis=1,name = \"concat__2\"\n",
    "        )\n",
    "        \n",
    "        user_id_embed = inputs['user_id']\n",
    "        if self._train:\n",
    "            user_id_embed = user_id_embed\n",
    "        else:\n",
    "            user_id_embed = user_id_embed\n",
    "\n",
    "        with tf.control_dependencies([item_tags, inputs['movie_id'], item_enhance]):\n",
    "            item_part = tf.concat([item_tags, inputs['movie_id'], item_enhance], axis = 1, name = \"concat_2\")\n",
    "        with tf.control_dependencies([multivalue_features_, user_enhance]):\n",
    "            user_part = tf.concat([multivalue_features_, user_enhance], axis = 1, name = \"concat_3\")\n",
    "        \n",
    "        #with tf.control_dependencies([user_id_embed, history_movies,user_enhance]):\n",
    "        #    user_part = tf.concat([user_id_embed, history_movies,user_enhance], axis = 1, name = \"concat_3\")\n",
    "\n",
    "        with tf.control_dependencies([user_part, item_part]): \n",
    "            hidden = tf.concat([user_part, item_part], axis=1,name = \"concat_4\")\n",
    "\n",
    "        #print(\"multi_value_part.shape\", multi_value_part.shape)\n",
    "        #print(\"categorical_part.shape\", categorical_part.shape)\n",
    "        #print(\"hidden.shape\", hidden.shape)\n",
    "        \n",
    "        hidden_1 = tf.compat.v1.layers.dense(hidden, 32, activation=tf.nn.leaky_relu, use_bias=True, name=\"fc_0\", reuse=tf.AUTO_REUSE)\n",
    "        hidden_2 = tf.compat.v1.layers.dense(hidden_1, 16, activation=tf.nn.leaky_relu, use_bias=True, name=\"fc_1\", reuse=tf.AUTO_REUSE)\n",
    "        outputs = tf.compat.v1.layers.dense(hidden_2, 1, activation=None, use_bias=True, name=\"logit\", reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class BaseLoss(metaclass=abc.ABCMeta):\n",
    "    \"\"\"Base class for a loss function component.\n",
    "\n",
    "    All subclasses of BaseLoss must override `loss_fn` method.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def loss_fn(self, logits: tf.Tensor, examples: tf.train.Example) -> tf.Tensor:\n",
    "        \"\"\"Build loss function.\n",
    "\n",
    "        :param logits: The input logits to build the loss function.\n",
    "        :param examples: The input `tf.Example` from which to abtain\n",
    "            all required data fields.\n",
    "        :return: A loss `tf.Tensor`. \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(BaseLoss):\n",
    "\n",
    "    def __init__(self, label_name):\n",
    "        self._label_name = label_name\n",
    "\n",
    "    def loss_fn(self, logits, examples):\n",
    "        print(examples[self._label_name])\n",
    "        labels = tf.expand_dims(\n",
    "            tf.to_float(tf.strings.to_number(examples[self._label_name])), 1, name=None)    \n",
    "        return self._cross_entropy_loss(logits, labels)\n",
    "\n",
    "    def _cross_entropy_loss(self, logits, labels):\n",
    "        \n",
    "        sample_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels\n",
    "        )\n",
    "        \n",
    "        avg_loss = tf.reduce_mean(sample_loss)\n",
    "        return avg_loss\n",
    "import json\n",
    "    \n",
    "\n",
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_iterator,\n",
    "                 cate_trans,\n",
    "                 test_iterator,\n",
    "                 transform_functions,\n",
    "                 train_fn,\n",
    "                 test_fn,\n",
    "                 validate_steps,\n",
    "                 log_steps,\n",
    "                 learning_rate,\n",
    "                 train_epochs=1,\n",
    "                 evaluator=None,\n",
    "                 save_checkpoints_dir=None,\n",
    "                 restore_checkpoint_path=None,\n",
    "                 validate_at_start=False,\n",
    "                 tensorboard_logdir=\"./\"):\n",
    "        \n",
    "        #self.neighbours = neighbours\n",
    "        self.train_iterator = train_iterator\n",
    "        self.cate_trans = cate_trans\n",
    "        self.test_iterator = test_iterator\n",
    "        self._transform_functions = transform_functions\n",
    "        self._train_fn = train_fn\n",
    "        self._test_fn = test_fn\n",
    "        self._train_epochs = train_epochs\n",
    "        self._save_checkpoints_dir = save_checkpoints_dir\n",
    "        self._restore_checkpoint_path = restore_checkpoint_path\n",
    "        self._validate_steps = validate_steps\n",
    "        self._log_steps = log_steps\n",
    "        self._learning_rate = learning_rate\n",
    "        self._validate_at_start = validate_at_start\n",
    "        self._evaluator = evaluator\n",
    "        self.total_loss = []\n",
    "        self.train_loss, self.train_logits, self.train_example, self.test_loss, self.test_logits, self.test_example, self._train_op = self._build_train_graph()\n",
    "        #self._valid_logger = ValidateLogger(tensorboard_logdir)\n",
    "        self._train_logger = TrainLogger(self._log_steps, tensorboard_logdir)\n",
    "\n",
    "    def run(self, sess=None):\n",
    "        self.total_loss = 0\n",
    "        if sess is None:\n",
    "            self._sess = self._create_and_init_session()\n",
    "        else:\n",
    "            self._sess = sess\n",
    "\n",
    "        self._train_loop()\n",
    "        \n",
    "        self.save_embeddings()\n",
    "        \n",
    "        if sess is None:\n",
    "            self._sess.close()\n",
    "            \n",
    "    def save_embeddings(self):\n",
    "        user_ids = self._sess.run(self.cate_trans._embedding_tables['user_id']).tolist()\n",
    "        item_ids = self._sess.run(self.cate_trans._embedding_tables['movie_id']).tolist()\n",
    "        \n",
    "        dict_ids = {\n",
    "            \"user_ids\":user_ids,\n",
    "            \"item_ids\":item_ids\n",
    "        }\n",
    "        \n",
    "        with open('ids.json', 'w') as outfile:\n",
    "            json.dump(dict_ids, outfile)\n",
    "\n",
    "    def _create_and_init_session(self):\n",
    "        NUM_THREADS = 200\n",
    "        sess = tf.Session(config=tf.ConfigProto(\n",
    "  intra_op_parallelism_threads=NUM_THREADS))\n",
    "        tf.global_variables_initializer().run(session=sess)\n",
    "        tf.tables_initializer().run(session=sess)\n",
    "        return sess\n",
    "\n",
    "    def _build_train_graph(self):\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        print(\"trainable_params\", trainable_params)\n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "        transform_fn = self._join_pipeline(self._transform_functions)\n",
    "        train_loss, train_logits, train_example = self._train_fn(transform_fn(self.train_iterator.get_next()))\n",
    "        test_loss, test_logits, test_example = self._test_fn(transform_fn(self.test_iterator.get_next()))\n",
    "        #self.neighbours = transform_fn(self._iterator.get_next())['neighbours']\n",
    "        grads = tf.gradients(train_loss, trainable_params)\n",
    "        train_op = trainer.apply_gradients(list(zip(grads, trainable_params)))\n",
    "        return train_loss, train_logits, train_example, test_loss, test_logits, test_example, train_op\n",
    "\n",
    "    def _train_loop(self):\n",
    "        step = 0\n",
    "        self.total_loss = []\n",
    "        epoch = 0\n",
    "        while epoch < self._train_epochs:\n",
    "            self.train_iterator.initializer.run(session = self._sess)\n",
    "            total_logits = []\n",
    "            total_labels = []\n",
    "            continue_ = True\n",
    "            while True:\n",
    "                success = self._train_step(epoch, step, total_logits, total_labels)\n",
    "                if not success:\n",
    "                    auc_ = self.cal_auc(total_logits, total_labels)\n",
    "                    print(\"auc_\", auc_)\n",
    "                    self._test_loop()\n",
    "                    if len(self.total_loss) > 2:\n",
    "                        if self.total_loss[-1] < self.total_loss[-2]:\n",
    "                            continue_ = False\n",
    "                            print(\"Overfitting\")\n",
    "                    break\n",
    "                step += 1\n",
    "            if not continue_:\n",
    "                break\n",
    "            epoch += 1\n",
    "            if epoch % 50 == 0:\n",
    "                self.save_embeddings()\n",
    "                #if step % self._validate_steps == 0:\n",
    "                #    self._validate(epoch=epoch, step=step)\n",
    "            #self._save_checkpoint(epoch + 1)\n",
    "\n",
    "    def _train_step(self, epoch, step, total_logits, total_labels):\n",
    "        try:\n",
    "            t_start = time.time()\n",
    "            train_loss, train_logits, train_example = self._sess.run([self.train_loss, self.train_logits, self.train_example, self._train_op])[0:3]\n",
    "            total_logits.append(train_logits)\n",
    "            total_labels.append(train_example)\n",
    "            \n",
    "            t_end = time.time()\n",
    "            self._train_logger.log_info(loss=train_loss,\n",
    "                                        time=t_end - t_start,\n",
    "                                        size=10,\n",
    "                                        epoch=epoch,\n",
    "                                        step=step + 1)\n",
    "            return True\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            return False\n",
    "        \n",
    "    def cal_auc(self, logits, example):\n",
    "        logits = np.array(logits).reshape(-1, 1)\n",
    "        example = np.array(example).reshape(-1, 1)\n",
    "        \n",
    "        #logits = np.concatenate(np.array(logits), axis = 0)\n",
    "        #example = np.concatenate(np.array(example), axis = 0)\n",
    "        \n",
    "        auc = roc_auc_score(y_true=example, y_score=logits)\n",
    "        return auc\n",
    "            \n",
    "    def _test_loop(self):\n",
    "        self.test_iterator.initializer.run(session = self._sess)\n",
    "        total_logits = []\n",
    "        total_labels = []\n",
    "        total_loss = []\n",
    "        while True:\n",
    "            #print(\"why\")\n",
    "            success = self._test_step(total_logits, total_labels, total_loss)\n",
    "            if success == -1:\n",
    "                auc_test = self.cal_auc(total_logits, total_labels)\n",
    "                print(\"test auc\", auc_test)\n",
    "                self.total_loss.append(auc_test)\n",
    "                break\n",
    "    \n",
    "    def _test_step(self,total_logits, total_labels, total_loss):\n",
    "        try:\n",
    "            test_loss, test_logits, test_example = self._sess.run([self.test_loss, self.test_logits, self.test_example])\n",
    "            total_logits.append(test_logits)\n",
    "            total_labels.append(test_example)\n",
    "            total_loss.append(test_loss)\n",
    "            return test_loss\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            return -1\n",
    "\n",
    "    def _save_checkpoint(self, step, prefix=\"ckpt_epoch\"):\n",
    "        if self._save_checkpoints_dir:\n",
    "            checkpoint_saver = tf.train.Saver(max_to_keep=None)\n",
    "            checkpoint_path = os.path.join(self._save_checkpoints_dir, prefix)\n",
    "            checkpoint_saver.save(self._sess, checkpoint_path, global_step=step)\n",
    "\n",
    "    def _restore_checkpoint(self):\n",
    "        #print(\"model path\",self._restore_checkpoint_path)\n",
    "        #and  os.listdir(self._restore_checkpoint_path)\n",
    "        if os.path.exists(self._restore_checkpoint_path):\n",
    "            checkpoint_saver = tf.train.Saver(max_to_keep=None)\n",
    "            checkpoint_saver.restore(self._sess, self._restore_checkpoint_path)\n",
    "\n",
    "    def _validate(self, epoch, step):\n",
    "        if self._evaluator is not None:\n",
    "            eval_results = self._evaluator.run(sess=self._sess)\n",
    "            self._valid_logger.log_info(eval_results, epoch=epoch, step=step)\n",
    "\n",
    "    def _join_pipeline(self, map_functions):\n",
    "        \n",
    "        def joined_map_fn(example):\n",
    "            for map_fn in map_functions:\n",
    "                example = map_fn(example)\n",
    "            return example\n",
    "\n",
    "        return joined_map_fn\n",
    "\n",
    "cate_features = ['age', 'gender', 'occupation']\n",
    "multivalue_features = ['hist_movie_id']\n",
    "item_features = ['movie_tags']\n",
    "nume_features = []\n",
    "\n",
    "train_network = MlpNetwork(\n",
    "    train = True,\n",
    "    cate_trans = cate_trans,\n",
    "    item_features = item_features,\n",
    "    categorical_features=cate_features,\n",
    "    numerical_features=nume_features,\n",
    "    multivalue_features=multivalue_features,\n",
    "    loss=CrossEntropyLoss(label_name = 'label')\n",
    ")\n",
    "\n",
    "test_network = MlpNetwork(\n",
    "    train = False,\n",
    "    cate_trans = cate_trans,\n",
    "    item_features = item_features,\n",
    "    categorical_features=cate_features,\n",
    "    numerical_features=nume_features,\n",
    "    multivalue_features=multivalue_features,\n",
    "    loss=CrossEntropyLoss(label_name = 'label')\n",
    ")\n",
    "\n",
    "class TrainLogger(object):\n",
    "\n",
    "    def __init__(self, log_steps, tensorboard_logdir=None):\n",
    "        self._log_steps = log_steps\n",
    "        self.last_epoch = 0\n",
    "        self.total = 0\n",
    "        self._tensorboard_writer = None\n",
    "        if tensorboard_logdir:\n",
    "            self._tensorboard_writer = tf.summary.FileWriter(tensorboard_logdir)\n",
    "        self._cleanup()\n",
    "\n",
    "    def log_info(self, loss, time, size, epoch, step):\n",
    "        self._total_loss += loss\n",
    "        self._total_time += time\n",
    "        self._total_size += size\n",
    "        self._total_steps += 1\n",
    "\n",
    "        if self._total_steps >= self._log_steps:\n",
    "            avg_loss = self._total_loss / self._total_steps\n",
    "            fps = self._total_size / float(self._total_time)\n",
    "            self._log_to_console(avg_loss, self._total_time, fps, epoch, step)\n",
    "            self._log_to_tensorboard(loss, fps, step)\n",
    "            self._cleanup()\n",
    "\n",
    "    def _log_to_console(self, loss, time, fps, epoch, step):\n",
    "        if self.last_epoch == epoch:\n",
    "            self.total += loss\n",
    "        else:\n",
    "            print(\"[Train] Epoch: %d\\tLoss: %.5f\\t\", epoch, self.total/1000)\n",
    "            self.total = 0\n",
    "        print(\n",
    "            \"[Train] Epoch: %d\\tStep: %d\\tLoss: %.5f\\tTime: %.2f\\tFPS: %d\"\n",
    "            % (epoch, step, loss, time, fps)\n",
    "        )\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "    def _log_to_tensorboard(self, loss, fps, step):\n",
    "        if self._tensorboard_writer:\n",
    "            summary = tf.Summary(\n",
    "                value=[tf.Summary.Value(tag=\"train_loss\", simple_value=loss),\n",
    "                       tf.Summary.Value(tag=\"train_fps\", simple_value=fps)]\n",
    "            )\n",
    "            self._tensorboard_writer.add_summary(summary, step)\n",
    "            self._tensorboard_writer.flush()\n",
    "\n",
    "    def _cleanup(self):\n",
    "        self._total_loss = 0\n",
    "        self._total_time = 0\n",
    "        self._total_instance = 0\n",
    "        self._total_size = 0\n",
    "        self._total_steps = 0\n",
    "\n",
    "\n",
    "class ValidateLogger(object):\n",
    "    def __init__(self, tensorboard_logdir=None):\n",
    "        self._tensorboard_writer = None\n",
    "        if tensorboard_logdir:\n",
    "            self._tensorboard_writer = tf.summary.FileWriter(tensorboard_logdir)\n",
    "\n",
    "    def log_info(self, metric_results, epoch, step):\n",
    "        self._log_to_console(metric_results, epoch, step)\n",
    "        self._log_to_tensorboard(metric_results, step)\n",
    "\n",
    "    def _log_to_console(self, metric_results, epoch, step):\n",
    "        epoch_total = 0\n",
    "        results_str = \"\\t\".join(\n",
    "            [\"%s: %s\" % (metric_name, metric_result)\n",
    "             for metric_name, metric_result in metric_results.items()]\n",
    "        )\n",
    "        print(\"[Validate] Epoch: %d\\tStep: %d\\t%s\" % (epoch, step, results_str))\n",
    "\n",
    "    def _log_to_tensorboard(self, metric_results, step):\n",
    "        if self._tensorboard_writer:\n",
    "            summary = tf.Summary(\n",
    "                value=[tf.Summary.Value(tag=(\"valid_%s\" % metric_name),\n",
    "                                        simple_value=metric_result.result)\n",
    "                       for metric_name, metric_result in metric_results.items()]\n",
    "            )\n",
    "            self._tensorboard_writer.add_summary(summary, step)\n",
    "            self._tensorboard_writer.flush()\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    train_iterator=iterator,\n",
    "    cate_trans = cate_trans,\n",
    "    test_iterator = test_iterator,\n",
    "    transform_functions=[cate_trans.transform_fn],\n",
    "    train_fn=train_network.train_fn,\n",
    "    test_fn = test_network.train_fn,\n",
    "    validate_steps=5,\n",
    "    log_steps=10,\n",
    "    learning_rate=0.005,\n",
    "    train_epochs=500,\n",
    "    save_checkpoints_dir=10,\n",
    "    restore_checkpoint_path=20,\n",
    "    tensorboard_logdir=10,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyBb_rDF-xKI"
   },
   "outputs": [],
   "source": [
    "#auc_ 0.9974448867513553\n",
    "#test auc 0.9859417127610063\n",
    "#test auc 0.990621\n",
    "#auc_ 0.9970889560539397\n",
    "#test auc 0.9887362768997808\n",
    "#auc_ 0.9998272457738399\n",
    "#test auc 0.9964203584405279\n",
    "\n",
    "#auc_ 0.999725004116234\n",
    "#test auc 0.9847685588551847\n",
    "#user item combined\n",
    "#auc_ 0.9999014938477996\n",
    "#test auc 0.9888321861816355\n",
    "\n",
    "0.9765\n",
    "(0.983025 + 0.9829226638492917 + 0.9813233946314516)/3\n",
    "# - 0.983025 + 0.96834 - 0.9829226638492917 + 0.9734312800631626 - 0.9813233946314516)/6 \n",
    "#(0.9840043662397732/ 0.9843278173647924/ 0.986869226965459）- 0.9764813519305374 / 0.981874454044821/0.9843495799561665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGB-1vYGxNw3"
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAd793Sr-xKM"
   },
   "outputs": [],
   "source": [
    "sparse_users = all_usrs[all_usrs < 30]\n",
    "sparse_user_df = merged_df[merged_df.user_id.isin(sparse_users.index)]\n",
    "sparse_user_df.to_csv(\"sparse_merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw7ZqzTl-xKN"
   },
   "outputs": [],
   "source": [
    "len(statistic.stats[\"user_id\"].values_top_k(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4Dm7NG7-xKP"
   },
   "outputs": [],
   "source": [
    "len(interest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBcPsmrD-xKR"
   },
   "outputs": [],
   "source": [
    "#print(len(merged_df.movie_id.unique()))\n",
    "user_ids_ = statistic.stats[\"movie_id\"].values_top_k(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IwVupWE-xKT"
   },
   "outputs": [],
   "source": [
    "#han_user\n",
    "import json\n",
    "\n",
    "with open('ids.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZTg-cju-xKW"
   },
   "outputs": [],
   "source": [
    "user_id_dict = {}\n",
    "user_ids = data['item_ids'][:-1]\n",
    "for i in range(len(user_ids)):\n",
    "    user_id_dict[user_ids_[i]] = user_ids[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMiArJ9x-xKX"
   },
   "outputs": [],
   "source": [
    "user_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff0LmyPH-xKa"
   },
   "outputs": [],
   "source": [
    "len(merged_df.movie_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyMRkDjU-xKc"
   },
   "outputs": [],
   "source": [
    "extract_mdf = movie_df[movie_df.movie_id.isin(merged_df.movie_id.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbCucj0O-xKd"
   },
   "outputs": [],
   "source": [
    "sorted_dict = sorted([(int(a),b) for a,b in user_id_dict.items()], key = lambda x : x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTtExNkV-xKf"
   },
   "outputs": [],
   "source": [
    "#user_df = pd.DataFrame(data={\n",
    "#    \"tags\":list(interest_dict.values()),\n",
    "#    \"embed\":[x[1] for x in sorted_dict],\n",
    "#},index=[x[0] for x in sorted_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Nu-lztU-xKh"
   },
   "outputs": [],
   "source": [
    "extract_mdf['item_embed'] = [x[1] for x in sorted_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWfCekte-xKi"
   },
   "outputs": [],
   "source": [
    "extract_mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbGx95Yp-xKk"
   },
   "outputs": [],
   "source": [
    "user_hist_dict = {}\n",
    "for u,u_hist in merged_df.groupby('movie_id'):\n",
    "    user_hist_dict[u] = u_hist.movie_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niBbzCv8-xKm"
   },
   "outputs": [],
   "source": [
    "user_hist_list = [a for a in list(user_hist_dict.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMgbYuWZ-xKo"
   },
   "outputs": [],
   "source": [
    "user_df['user_hist'] = user_hist_list\n",
    "user_df['hist_len'] = user_df.user_hist.apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAfoActK-xKq"
   },
   "outputs": [],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFO93w4x-xKr"
   },
   "outputs": [],
   "source": [
    "new_column = [x[1] for x in sorted_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrFzWO12-xKt"
   },
   "outputs": [],
   "source": [
    "user_df[user_df.index == 1]['embed'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aALgQrnT-xKv"
   },
   "outputs": [],
   "source": [
    "def sort_lines(df, id_, name, user, advanced, version):\n",
    "    target_line = df[df.movie_id == id_][name].values[0]\n",
    "    simila = []\n",
    "    for i in range(df.shape[0]):\n",
    "        line_ = df.iloc[i][name]\n",
    "        simila.append((df.iloc[i].movie_id, np.dot(target_line, line_)/(np.linalg.norm(target_line)*np.linalg.norm(line_))))\n",
    "    #df['sort'] = simila\n",
    "    #df.sort_values(by = 'sort', inplace = True, ascending = False)\n",
    "    simila = sorted(simila, key = lambda x:x[1], reverse = True)\n",
    "    return [x[0] for x in simila[:5]]\n",
    "    \"\"\"\n",
    "    advan = \"normal\"\n",
    "    if advanced:\n",
    "        advan = \"advan\"\n",
    "    else:\n",
    "        advan = \"normal\"\n",
    "    if user:\n",
    "        df.to_csv(\"user_{}_similarity_{}_version{}.csv\".format(id_, advan, version))\n",
    "    else:\n",
    "        df.to_csv(\"item_{}_similarity_{}_version{}.csv\".format(id_, advan, version))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONLsMTaf-xKx"
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in range(extract_mdf.shape[0]):\n",
    "    m_id = extract_mdf.iloc[i].movie_id\n",
    "    res[m_id] = sort_lines(extract_mdf, m_id, 'item_embed', False, True, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yl_N2asi-xKy"
   },
   "outputs": [],
   "source": [
    "#delete the test dataset info to prevent data-leakage\n",
    "test_index = []\n",
    "for u_id, hist in merged_df.groupby('user_id'):\n",
    "    test_index.append(hist.iloc[-1].name)\n",
    "train_merged_df = merged_df[~merged_df.index.isin(test_index)]\n",
    "\n",
    "#create a itemcf user_item matrix\n",
    "max_item = max(train_merged_df.movie_id.unique() + 1)\n",
    "max_user = max(train_merged_df.user_id.unique() + 1)\n",
    "co_matrix = np.zeros((max_item,max_user))\n",
    "\n",
    "print(co_matrix.shape)\n",
    "\n",
    "for row_num in range(train_merged_df.shape[0]):\n",
    "    row = train_merged_df.iloc[row_num]\n",
    "    co_matrix[row.movie_id, row.user_id] = row.rating\n",
    "\n",
    "def cos_value(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "movie_id_list = train_merged_df.movie_id.value_counts().index\n",
    "simi_dict = {}\n",
    "for item in tqdm(movie_id_list):\n",
    "    temp_list = []\n",
    "    for other_item in movie_id_list:\n",
    "        temp_list.append((other_item, cos_value(co_matrix[item], co_matrix[other_item])))\n",
    "    temp_list = sorted(temp_list, key = lambda x:x[1], reverse = True)\n",
    "    temp_list = [x[0] for x in temp_list[:10]]\n",
    "    simi_dict[item] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A454NOqm-xK0"
   },
   "outputs": [],
   "source": [
    "simi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWxhP6Ez-xK1"
   },
   "outputs": [],
   "source": [
    "movie_1 = 741\n",
    "movie_2 = 3858\n",
    "print(len(set(merged_df[merged_df.movie_id == movie_1].user_id.values)))\n",
    "print(len(set(merged_df[merged_df.movie_id == movie_2].user_id.values)))\n",
    "set(merged_df[merged_df.movie_id == movie_1].user_id.values).intersection(set(merged_df[merged_df.movie_id == movie_2].user_id.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp3tB9lM-xK3"
   },
   "outputs": [],
   "source": [
    "merged_df.movie_id.value_counts().index[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Td9piTe1-xK5"
   },
   "outputs": [],
   "source": [
    "movie_df[movie_df.movie_id.isin(simi_dict[301])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQGzKYfP-xK8"
   },
   "outputs": [],
   "source": [
    "new_res = {}\n",
    "for i in res:\n",
    "    new_res[str(i)] = str(list(res[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GwR8TqJ-xK-"
   },
   "outputs": [],
   "source": [
    "len(extract_mdf.movie_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-QkHA6J-xLA"
   },
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "with open('similar_items.json', 'w') as outfile:\n",
    "    json.dump(simi_dict, outfile, cls=NumpyEncoder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBuTV4ZM-xLB"
   },
   "outputs": [],
   "source": [
    "sort_lines(extract_mdf, 1, 'item_embed', False, True, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpPrRUUW-xLF"
   },
   "outputs": [],
   "source": [
    "#print(len(statistic.stats[\"user_id\"].values_top_k(None)))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "res = pca.fit_transform(data['item_ids'])\n",
    "\n",
    "plt.figure()\n",
    "#lw = 2\n",
    "\n",
    "#for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "plt.scatter(res[:, 0], res[:, 1],alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlEn2Y-O-xLG"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEirteIL-xLH"
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QLP2EI1-xLJ"
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MfBWmjm-xLK"
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FupCkXF-xLM"
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKidapjG-xLN"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #initilize the iterator\n",
    "    iterator.initializer.run()\n",
    "    while True:\n",
    "        try:\n",
    "            ne = iterator.get_next()\n",
    "            read_features = dict()\n",
    "            for key in train_dataset.keys():\n",
    "                if key == 'hist_movie_id' or key == 'neighbours' :\n",
    "                    read_features[key] = tf.FixedLenSequenceFeature([],dtype=tf.string,allow_missing=True)\n",
    "                else:\n",
    "                    read_features[key] = tf.FixedLenFeature([], dtype=tf.string)\n",
    "\n",
    "            # Extract features from serialized data\n",
    "            read_data = tf.parse_example(serialized=ne,\n",
    "                                                features=read_features)\n",
    "\n",
    "            for k, v in read_data.items():\n",
    "                read_data[k] = tf.identity(v, name=k)\n",
    "            print(read_data['user_id'].eval())\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2TtdNga-xLO"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset('train.tfrecord')\n",
    "dataset = dataset.batch(1000, True)\n",
    "#dataset = _dataset_map(dataset, [_transform])\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "with tf.Session() as sess:\n",
    "    #initilize the iterator\n",
    "    iterator.initializer.run()\n",
    "    while True:\n",
    "        try:\n",
    "            ne = iterator.get_next()\n",
    "            read_features = dict()\n",
    "            for key in ['user_tags', 'movie_tags', 'user_id_offline', 'user_id_offline_fixed', 'age', 'gender', 'hist_movie_id', 'label', 'movie_id', 'neighbours', 'occupation', 'user_id']:\n",
    "                if key == 'hist_movie_id' or key == 'neighbours' or key == 'user_tags' or key == 'movie_tags':\n",
    "                    read_features[key] = tf.FixedLenSequenceFeature([],dtype=tf.string,allow_missing=True)\n",
    "                else:\n",
    "                    read_features[key] = tf.FixedLenFeature([], dtype=tf.string)\n",
    "\n",
    "            # Extract features from serialized data\n",
    "            read_data = tf.parse_example(serialized=ne,\n",
    "                                                features=read_features)\n",
    "\n",
    "            for k, v in read_data.items():\n",
    "                read_data[k] = tf.identity(v, name=k)\n",
    "            print(read_data['movie_tags'].eval())\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkLEBzEJ-xLQ"
   },
   "outputs": [],
   "source": [
    "data_handler.create_next_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLPAt41c-xLR"
   },
   "outputs": [],
   "source": [
    "gen_data = get_single_data(train_dataset, data_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbhPEsWn-xLT"
   },
   "outputs": [],
   "source": [
    "next(gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ku3_riyc-xLU"
   },
   "outputs": [],
   "source": [
    "type2movie = {}\n",
    "for i in range(movie_df.shape[0]):\n",
    "    movie = movie_df.iloc[i]\n",
    "    for type_ in movie.Genres:\n",
    "        if type_ in type2movie:\n",
    "            type2movie[type_].append(movie.MovieID)\n",
    "        else:\n",
    "            type2movie[type_] = [movie.MovieID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qXJ8stE-xLV"
   },
   "outputs": [],
   "source": [
    "[(i, len(type2movie[i])) for i in type2movie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCd533xY-xLX"
   },
   "outputs": [],
   "source": [
    "item2neighbours = {}\n",
    "for i in range(movie_df.shape[0]):\n",
    "    movie = movie_df.iloc[i]\n",
    "    total_neigh = set()\n",
    "    for type_ in movie.Genres:\n",
    "        total_neigh = total_neigh.union(set(type2movie[type_]))\n",
    "    item2neighbours[movie.MovieID] = total_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fRbP9TC-xLY"
   },
   "outputs": [],
   "source": [
    "nb_nei = [(i, len(item2neighbours[i])) for i in item2neighbours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g1nimix-xLa"
   },
   "outputs": [],
   "source": [
    "pd.Series([i[1] for i in nb_nei]).hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlQLrs53-xLb"
   },
   "outputs": [],
   "source": [
    "user_df.UserID = user_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSw2PUYi-xLc"
   },
   "outputs": [],
   "source": [
    "age2user = {}\n",
    "occ2user = {}\n",
    "for i in range(user_df.shape[0]):\n",
    "    user = user_df.iloc[i]\n",
    "    if user.Age in age2user:\n",
    "        age2user[user.Age].append(user.UserID)\n",
    "    else:\n",
    "        age2user[user.Age] = [user.UserID]\n",
    "    if user.Age in occ2user:\n",
    "        occ2user[user.Occupation].append(user.UserID)\n",
    "    else:\n",
    "        occ2user[user.Occupation] = [user.UserID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7dJ3lae-xLd"
   },
   "outputs": [],
   "source": [
    "user2neighbours = {}\n",
    "for i in range(user_df.shape[0]):\n",
    "    user = user_df.iloc[i]\n",
    "    total_neigh = set()\n",
    "    for age_ in [user.Age]:\n",
    "        total_neigh = total_neigh.union(set(age2user[age_]))\n",
    "    for occ_ in [user.Occupation]:\n",
    "        total_neigh = total_neigh.union(set(occ2user[occ_]))\n",
    "    user2neighbours[user.UserID] = total_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJwNt6R7-xLf"
   },
   "outputs": [],
   "source": [
    "pd.Series([len(user2neighbours[a]) for a in user2neighbours]).hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4w2rMRK-xLg"
   },
   "outputs": [],
   "source": [
    "user2neighbours = {str(a): str(user2neighbours[a]) for a in user2neighbours}\n",
    "item2neighbours = {str(b): str(item2neighbours[b]) for b in item2neighbours}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKNIbJN7-xLi"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('user2neighbours', 'w') as json_file:\n",
    "    json.dump(user2neighbours, json_file)\n",
    "with open('item2neighbours', 'w') as json_file:\n",
    "    json.dump(item2neighbours, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdK6NAZN-xLj"
   },
   "outputs": [],
   "source": [
    "user_df.UserID.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZaYV9IX-xLk"
   },
   "outputs": [],
   "source": [
    "rating_df.UserID = rating_df.UserID.apply(lambda x:x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XskBtW9--xLm"
   },
   "outputs": [],
   "source": [
    "movie_df.MovieID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rORQVIw3-xLn"
   },
   "outputs": [],
   "source": [
    "movie_df_ = pd.read_csv(\"movies.dat\", sep=\"::\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ng424GIK-xLp"
   },
   "outputs": [],
   "source": [
    "movieID_map = {b:a for (a,b) in movie_df_.iloc[:,0].to_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Las3oEso-xLq"
   },
   "outputs": [],
   "source": [
    "rating_df.MovieID = rating_df.MovieID.map(movieID_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQLnNaFb-xLv"
   },
   "outputs": [],
   "source": [
    "rating_df.UserID = rating_df.UserID.apply(lambda x : x - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaMrUp4A-xLx"
   },
   "outputs": [],
   "source": [
    "sorted_rating = rating_df.sort_values(by = \"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGywFQM6-xLz"
   },
   "outputs": [],
   "source": [
    "sorted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXFTk4eM-xL1"
   },
   "outputs": [],
   "source": [
    "y = sorted_rating.Rating\n",
    "X = sorted_rating.drop(['Rating'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "io1ALYQO-xL2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(sorted_rating, test_size=0.33, random_state=42, stratify = sorted_rating[\"UserID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHDKEHh--xL4"
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"train_data.csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_17DKBo-xL6"
   },
   "outputs": [],
   "source": [
    "X_test.to_csv(\"test_data.csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWWun2ps-xL8"
   },
   "outputs": [],
   "source": [
    "len(X_test.UserID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqEwpO5p-xMF"
   },
   "outputs": [],
   "source": [
    "X_train.sort_values(by = 'UserID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JU9iAL07-xMG"
   },
   "outputs": [],
   "source": [
    "670140/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NOqFp4g-xMH"
   },
   "outputs": [],
   "source": [
    "X_test.sort_values(by = 'UserID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm8wZ2Fs-xMI"
   },
   "outputs": [],
   "source": [
    "rating_train = sorted_rating.iloc[:int(sorted_rating.shape[0] * 2/3) , :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wqc2jil9-xMJ"
   },
   "outputs": [],
   "source": [
    "rating_train.to_csv(\"train_data.csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNfHvpkH-xML"
   },
   "outputs": [],
   "source": [
    "rating_test = sorted_rating.iloc[-int(sorted_rating.shape[0] * 1/3): , :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djIxI3dO-xMM"
   },
   "outputs": [],
   "source": [
    "rating_test.to_csv(\"test_data.csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDN1GABi-xMO"
   },
   "outputs": [],
   "source": [
    "rating_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27JOzdkF-xMP"
   },
   "outputs": [],
   "source": [
    "np.ones((1,300,1)) + np.ones((1,1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cFLmuu8-xMQ"
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"train_data.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caGYCnNw-xMR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "prepro.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
